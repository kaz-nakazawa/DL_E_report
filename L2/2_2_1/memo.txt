誤差逆伝播法が下位層に進んでいくに連れて、勾配がどんどん緩やかになり、
勾配降下法によるパラメータ更新では下位層のパラメータはほとんど変わらず、
訓練は最適値に収束しなくなる。重みの初期値設定

Xavierの初期値を設定する際の活性化関数
?ReLU関数
?シグモイド（ロジスティック）関数
?双曲線正接関数
設定方法
?重みの要素を、前の層のノード数の平方根で除算した値

He
　Heの初期値を設定する際の活性化関数
　　?Relu関数

　初期値の設定方法
　　?重みの要素を、前の層のノード数の平方根で除算した値に対し√２をかけ合わせた値
　　
　　バッチ正規化とは？ 
　　　　　ミニバッチ単位で、入力値のデータの偏りを抑制する手法
　　バッチ正規化の使い所とは？
		活性化関数に値を渡す前後に、バッチ正規化の処理を孕んだ層を加える
　　バッチ正規化層への入力値は
または
数式
S1)-3

1) 学習率の値が大きい場合
    ・最適値にいつまでもたどり着かず発散してしまう。
2) 学習率の値が小さい場合
　　・発散することはないが、小さすぎると収束するまでに時間がかかってしまう。
　　・大域局所最適値に収束しづらくなる

　⇒　学習率最適化手法を利用して学習率を最適化
　
　学習率最適化手法
　
　
　誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する
　メリット
・局所的最適解にはならず、大域的最適解となる。
・谷間についてから最も低い位置(最適値)にいくまでの時間が早い。
モメンタム
勾配降下法
誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する
誤差をパラメータで微分したものと学習率の積を減算する
S2)-1
数式 誤差をパラメータで微分したものと
再定義した学習率の積を減算する
日本語
45


AdaGradのメリット
・課題勾配の緩やかな斜面に対して、最適値に近づける
学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった。
AdaGradのメリット

・モメンタムの、過去の勾配の指数関数的減衰平均
・RMSPropの、過去の勾配の2乗の指数関数的減衰平均
上記をそれぞれ孕んだ最適化アルゴリズムである。

S2)-3


テスト誤差と訓練誤差とで学習曲線が乖離すること。
特定の訓練サンプルに対して、特化して学習する

原因
ネットワークの自由度(層数、ノード数、パラメータの値etc…)が高い

?パラメータの数が多い
?パラメータの値が適切でない
?ノードが多いetc…

ネットワークの自由度(層数、ノード数、パラメータの値etc…)が高い

・ネットワークの自由度(層数、ノード数、パラメータの値etc…)を制約すること
・正則化手法を利用して過学習を抑制する

荷重減衰
  重みが大きい値をとることで、過学習が発生することがある。
  誤差に対して、正則化項を加算することで、重みを抑制する。
正則化項 : 誤差関数に、Pノルムを加える。 pノルムの

p = 1の場合、 L1正則化と呼ぶ。
p = 2の場合、 L2正則化と呼ぶ。

ドロップアウトとは？
・ノードの数が多いと過学習が発生することがある。ランダムにノードを削除して学習させること。
ドロップアウトとは？
・ランダムにノードを削除して学習させること
メリットとして
・データ量を変化させずに、異なるモデルを学習させていると解釈できる
過学習の課題


主に画像に関するモデルを作成することに用いられる。
畳み込み層とプーリング層、全結合層から構成される。
入力の加工によっては、音声等の時系列データに対しても適用できる。
LesNet → CNN のオリジナル

CNNは一般的な順伝播型のニューラルネットワークとは違い、
畳み込み層(Convolution Layer)とプーリング層(Pooling Layer)と全結合層から構成されるニューラルネットワーク。

CNNとは、畳み込み層とプーリング層という二つの特殊なレイヤーを持ったニューラルネットワークです。

対象領域のMax値または、平均値を取得し出力値とする。

対象領域のMax値または、平均値を取得

入力画像の周囲を固定のデータで埋めること。

 1) AlexNet概要
  ・AlexNetとは2012年に開かれた画像認識コンペティション2位に大差をつけて優勝したモデルである。
  ・AlexNetの登場で、ディープラーニングが大きく注目を集めた。
 2) モデルの構造
    5層の畳み込み層およびプーリング層とそれに続く3層の全結合層から構成される。

 3) 過学習対策
   ・サイズ4096の全結合層の出力にドロップアウトを使用
   
   sigmoid(x)  の導関数は  (1?sigmoid(x))・sigmoid(x)
   モメンタム
   　・メリット
・局所的最適解にはならず、大域的最適解となる。
・谷間に着いてから最も低い位置(最適値)にいくまでの時間が早い。

b) AdaGrad
    ・誤差をパラメータで微分したものと再定義した学習率の積を減算する。
・メリット
       ・勾配の緩やかな斜面に対して、最適値に近づける
・課題
・学習率が徐々に小さくなるので、鞍点問題が起こる事がある。

c) RMSProp
・誤差をパラメータで微分したものと再定義した学習率の積を減算する。
・メリット
       ・局所的最適解にはならず、大域的最適解となる。
・ハイパーパラメータの調整が必要な場合が少ない。

d) Adam
・モメンタムの、過去の勾配の指数関数的減衰平均
RMSPropの、過去の勾配の2乗の指数関数的減衰平均
上記をそれぞれ孕んだ最適化アルゴリズム。
・メリット
        モメンタム, RMSPropのメリットを持つ。
学習を速く進行させることができる（学習係数を大きくすることができる）

課題2: 活性化関数 Sigmoid で、初期値設定 He 

[try] 活性化関数と重みの初期化方法を変えてみよう
初期状態ではsigmoid - gauss
activationはReLU、weight_init_stdは別の数値や'Xavier'・'He'に変更可能

[try] バッチ正規化をしてみよう

optimizer SGD

SGD
Momentum
MomentumをもとにAdaGradを作ってみよう
RSMprop
Adam
[try] 学習率を変えてみよう
[try] 活性化関数と重みの初期化方法を変えてみよう
初期状態ではsigmoid - gauss
[try] バッチ正規化をしてみよう

2_4_optimizer_01_SGD.py
2_4_optimizer_02_Momentum.py
2_4_optimizer_03_AdaGrad.py
2_4_optimizer_04_RSMprop.py
2_4_optimizer_05_Adam.py
2_4_optimizer_Adam_02_学習率変更.py
2_4_optimizer_Adam_03_ReLU_avier.py
2_4_optimizer_Adam_04_バッチ正規化.py

[try] バッチ正規化をしてみよう


sigmoid - gauss
ReLU - gauss
sigmoid - Xavier
ReLU - He
課題1: 隠れ層のサイズを変更させる
課題2: 活性化関数 Sigmoid で、初期値設定 He 
課題3: 活性化関数 Relu で、初期値設定 Xiavier

overfiting
weight decay L2
weight decay L1
try] weigth_decay_lambdaの値を変更して正則化の強さを確認しよう
Dropout
[try] dropout_ratioの値を変更してみよう
[try] optimizerとdropout_ratioの値を変更してみよう
Dropout + L1


column to image
mage to column	2_6_simple_CNN_im2col_01.py
[try] im2colの処理を確認しよう	2_6_simple_CNN_im2col_02.py
関数内でtransposeの処理をしている行をコメントアウトして下のコードを実行してみよう	2_6_simple_CNN_im2col_02.py
・input_dataの各次元のサイズやフィルターサイズ・ストライド・パディングを変えてみよう	2_6_simple_CNN_im2col_03.py


1_1_forward_propagation.ipynb
